{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Dependency Management",
        "description": "Set up the project repository, define the basic project structure, and add necessary dependencies, including pinning the 'formulas' library to version 1.2.2.",
        "details": "Initialize a git repository. Create project directories (e.g., `src`, `tests`, `docs`). Create a `requirements.txt` or equivalent dependency file. Add `fastapi`, `uvicorn`, `pandas`, `openpyxl`, `xlrd`, `formulas==1.2.2` (ensure compatibility with pandas/openpyxl), and any other required libraries. Set up a virtual environment.",
        "testStrategy": "Verify that all specified dependencies are installed correctly in the virtual environment and that the 'formulas' library is specifically version 1.2.2. Run a simple import test for key libraries.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement File Upload and Temporary Storage",
        "description": "Implement the file upload mechanism for both the CLI and REST API interfaces, handling incoming files and storing them temporarily.",
        "details": "For the REST API, use FastAPI's `UploadFile` feature to accept POST requests with file data. For the CLI, use `argparse` or similar to accept a file path argument. Implement logic to save the uploaded/specified file to a temporary directory with a unique identifier. Ensure support for `.xlsx`, `.csv`, and `.tsv` extensions.",
        "testStrategy": "Test file uploads via both CLI and API for each supported file type (.xlsx, .csv, .tsv). Verify that files are correctly received and stored temporarily with their original names or a unique identifier.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Shared File Validation Utilities",
            "description": "Create reusable functions for validating uploaded files, including checking file type (.xlsx, .csv, .tsv) based on extension/mime type and enforcing the file size limit (<10MB). Include error handling for invalid types and sizes.",
            "dependencies": [],
            "details": "Implement validation logic as per F1 and F3. Write unit tests for validation functions covering valid/invalid file types and sizes.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CLI File Upload and Validation",
            "description": "Develop the CLI interface to accept a file path as input. Use the shared validation utilities (Subtask 1) to check the file before processing. Implement error handling for validation failures and file system errors. Include test cases for CLI file input.",
            "dependencies": [
              1
            ],
            "details": "Implement CLI logic using argparse or typer (as per RULES.md). Integrate validation from Subtask 1. Add tests for CLI argument parsing and validation error handling.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement API File Upload Endpoint with Validation",
            "description": "Create a REST API endpoint using FastAPI to accept file uploads via multipart form data. Use the shared validation utilities (Subtask 1) to check the uploaded file. Implement error handling for validation failures and API-specific issues. Include test cases for the API endpoint.",
            "dependencies": [
              1
            ],
            "details": "Implement FastAPI endpoint as per F15. Handle multipart file uploads. Integrate validation from Subtask 1. Add integration tests for the API endpoint covering successful uploads and validation errors.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement File Parsing for Supported Types",
        "description": "Develop the core logic for parsing uploaded files based on their type (.xlsx, .csv, .tsv) to read data from all sheets/sections.",
        "details": "Use `pandas` with appropriate engines (`openpyxl` for .xlsx, built-in pandas readers for .csv/.tsv) to read data. For .xlsx, iterate through all sheets in the workbook. For .csv/.tsv, treat the single file as one sheet. Store the parsed data in a structured format (e.g., dictionary of pandas DataFrames, where keys are sheet names).",
        "testStrategy": "Prepare test files for each supported type (.xlsx with multiple sheets, .csv, .tsv). Parse these files and verify that all sheets/data are read correctly into the expected data structure. Check for correct handling of different delimiters for .csv/.tsv.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Excel File Parsing and Formula Identification",
            "description": "Develop the module responsible for parsing .xls and .xlsx files using the 'formulas' library. Identify cells containing formulas and extract their raw string representation. Integrate logging for parsing activities and potential warnings.",
            "dependencies": [],
            "details": "Utilize 'formulas.ExcelModel' for parsing. Handle different sheet types and structures within Excel files. Implement logic to iterate through cells and identify those marked as containing formulas by the library. Capture the raw formula string from identified cells. Adhere to project naming conventions and code organization standards. Implement logging for successful parsing, sheet processing, and initial formula identification.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CSV/TSV File Parsing and Potential Formula Identification",
            "description": "Develop the module responsible for parsing .csv and .tsv files using 'pandas'. Identify cells containing strings that potentially represent formulas (e.g., starting with '='). Integrate logging for parsing activities.",
            "dependencies": [],
            "details": "Use 'pandas.read_csv' with appropriate delimiters (comma, tab). Implement logic to iterate through DataFrame cells and identify strings that start with '=' as potential formulas. Note that CSV/TSV do not have native formula types like Excel, so this is based on string content. Adhere to project naming conventions and code organization standards. Implement logging for successful parsing and potential formula string identification.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Unified Formula Extraction and Data Preparation",
            "description": "Develop the core logic to process the identified formula information from both Excel (Subtask 1) and CSV/TSV (Subtask 2) sources. Prepare the extracted data (cell reference, sheet name, formula string, source file) for inclusion in the unified data structure.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create a processing layer that accepts input from the Excel parsing module (Subtask 1) and the CSV/TSV parsing module (Subtask 2). Extract the necessary details (sheet name, cell address, raw formula string) in a consistent format. Handle the differences in how cell references and sheet names are provided by the underlying libraries/methods. Prepare data objects or dictionaries suitable for the unified structure defined in Subtask 4. Integrate logging for the extraction process.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Design and Implement Unified Data Structure and Output Mechanism",
            "description": "Define and implement the data structure that will hold the extracted formula information from all supported file types (Excel, CSV, TSV). Implement the mechanism to output this unified data structure.",
            "dependencies": [
              3
            ],
            "details": "Design a Python class or a standardized dictionary structure to represent each extracted formula instance. This structure must include fields for 'file_path', 'sheet_name', 'cell_address', 'formula_string', and 'error' (to capture issues during parsing/extraction). Ensure the structure is consistent regardless of the source file type. Implement a method or function to collect these instances and provide them as a list or similar aggregate structure. Adhere to project naming conventions and code organization standards.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Comprehensive Error Handling and Testing Suite",
            "description": "Implement robust error handling for parsing issues (corrupted files, unsupported features) and extraction issues (invalid formulas, edge cases). Develop and execute a comprehensive testing suite, including unit tests (targeting 90%+ coverage), integration tests, benchmark tests using 5 specified Excel files, and tests for various edge cases (empty files, files with no formulas, malformed CSV/TSV formula strings).",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Implement try-except blocks to catch exceptions raised by 'formulas' and 'pandas' during parsing and by the extraction logic (Subtask 3). Define how errors are captured and stored in the 'error' field of the unified data structure (Subtask 4). Ensure logging captures detailed error information. Write unit tests for each function/method in modules developed in Subtasks 1, 2, 3, and 4, aiming for at least 90% code coverage. Create integration tests to verify the end-to-end flow from file ingestion to unified data structure output. Prepare 5 benchmark Excel files and test the extraction performance and correctness against them. Create test cases for edge scenarios like corrupted files, files with only data (no formulas), files with unusual characters in formulas, and CSV/TSV files with strings that might ambiguously start with '='. Document the error handling strategy and test coverage results.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate Formulas Library for Extraction",
        "description": "Integrate the 'formulas' library (v1.2.2) to extract formulas from the parsed Excel sheets.",
        "details": "After parsing the file into a pandas DataFrame structure per sheet, iterate through cells in .xlsx sheets using `openpyxl` (or equivalent access via pandas if possible) to identify cells containing formulas. Use the `formulas` library's parsing capabilities to extract the formula string and potentially an initial representation. Focus on extracting the formula string itself at this stage.",
        "testStrategy": "Create test .xlsx files with various formulas in different cells and sheets. Run the extraction logic and verify that the formula strings from all formula-containing cells across all sheets are correctly identified and extracted using the `formulas` library.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract and Sanitize Headers",
            "description": "Implement logic to extract potential variable names from spreadsheet headers. Sanitize extracted headers by converting to lowercase, replacing spaces with underscores, and removing/handling special characters.",
            "dependencies": [],
            "details": "Handle merged cells, empty headers, and headers containing non-alphanumeric characters. Ensure output is suitable for use as a base for Python variable names.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Primary Name Generation",
            "description": "Develop algorithms to generate variable names based on sanitized headers and potentially Named Ranges, adhering strictly to Python's snake_case convention.",
            "dependencies": [
              1
            ],
            "details": "Prioritize Named Ranges if available (as per F6). Apply snake_case formatting to sanitized headers. Handle potential conflicts with Python reserved keywords by appending a suffix (e.g., '_var').",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Fallback Name Generation",
            "description": "Develop the fallback strategy for cells or ranges that do not have associated headers or Named Ranges, generating names in the format `cell_<col><row>` (e.g., `cell_A1`).",
            "dependencies": [
              1
            ],
            "details": "This strategy applies when primary methods (headers, Named Ranges) fail or are unavailable. Ensure the generated format `cell_<col><row>` is consistently applied.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Enforce Variable Name Uniqueness",
            "description": "Implement a mechanism to ensure all generated variable names across the entire workbook are unique, regardless of their source (header, Named Range, fallback).",
            "dependencies": [
              2,
              3
            ],
            "details": "Detect duplicate names resulting from different sources or ambiguous/duplicate headers (as per F10). Resolve duplicates by appending a unique suffix (e.g., '_1', '_2') to the generated name. Log instances of duplicate name resolution (as per F17).",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Develop Variable Naming Logic",
        "description": "Implement the logic to determine variable names for cells based on the specified hierarchy: Named Range > Header (1st row) > Cell Reference.",
        "details": "For each cell containing a formula or value that will be referenced: Check if it has a defined Excel Named Range. If not, check if it's in a column with a header in the first row. If neither, use the cell reference format (e.g., `cell_A1`). Handle ambiguities like merged headers by flattening and suffixing (e.g., `Revenue_Q2_1`). Log warnings for empty or malformed headers leading to fallback.",
        "testStrategy": "Create test sheets covering all naming scenarios: named ranges, clear headers, merged headers, empty headers, and no clear headers. Verify that the generated variable names match the expected output for each case according to the specified hierarchy and ambiguity rules. Check that warnings are logged correctly for fallbacks.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Formula Parsing and Syntax Conversion",
            "description": "Develop logic to parse Excel formula strings, convert operators (+, -, *, /, ^, etc.) and basic syntax (parentheses, string literals) into equivalent Python syntax.",
            "dependencies": [],
            "details": "Include handling of operator precedence, implicit multiplication (if applicable), and different literal types (numbers, strings, booleans). Test with formulas involving various operators and nested parentheses.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop Excel Function Mapping",
            "description": "Create a mapping from common Excel built-in functions (e.g., SUM, AVERAGE, IF, VLOOKUP) to corresponding Python functions or custom implementations.",
            "dependencies": [
              1
            ],
            "details": "Define how arguments are passed and results are handled for each mapped function. Consider common functions first. Test mapping with different function calls and argument types.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Cell Reference Resolution",
            "description": "Develop logic to identify cell references (e.g., A1, B2:C5) within the parsed formula and replace them with corresponding Python variable names based on a provided mapping.",
            "dependencies": [
              1
            ],
            "details": "Handle absolute ($A$1) and relative (A1) references. Support single cell references and potentially range references. Test resolution with various reference types and locations in formulas.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Generate Python Code Structure",
            "description": "Assemble the parsed and resolved formula components into a complete Python code snippet, including necessary imports, variable assignments for inputs, and the final calculation expression.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create a function or script structure. Assign input variables based on cell reference resolution. Include imports for any required libraries. Test the execution of generated code with sample inputs and expected outputs.",
            "status": "in-progress",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Handle Unsupported Excel Functions",
            "description": "Identify Excel functions encountered during parsing that do not have a defined mapping and implement a mechanism to report these as warnings or errors.",
            "dependencies": [
              2
            ],
            "details": "Log unsupported functions. Potentially insert placeholder comments or raise specific exceptions in the generated code. Test with formulas containing known unsupported functions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Format Generated Python Code",
            "description": "Apply standard Python code formatting rules (e.g., using a linter like Black or autopep8) to the generated code for improved readability and consistency.",
            "dependencies": [
              4
            ],
            "details": "Ensure proper indentation, spacing, and line breaks. Add comments where necessary. Verify formatting compliance using a linter.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Handle Cross-Sheet References",
        "description": "Add support for handling cross-sheet references within formulas.",
        "details": "When parsing formulas using the `formulas` library, identify references that point to cells in other sheets (e.g., `Sheet2!A1`). Ensure the variable naming logic (Task 5) can correctly generate names for these referenced cells in other sheets. The script generation (Task 8) must then correctly link these cross-sheet variables.",
        "testStrategy": "Create test .xlsx files with formulas referencing cells in other sheets. Verify that the parser correctly identifies these references and that the naming logic generates appropriate variable names for the referenced cells in the target sheets.",
        "priority": "medium",
        "dependencies": [
          4,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Unsupported Formula Placeholders",
        "description": "Implement the handling of unsupported formulas by substituting them with placeholder values or comments in the generated Python script.",
        "details": "Identify formulas that the `formulas` library cannot parse or that are explicitly out of scope (e.g., `INDIRECT`, `OFFSET`, external links). For such formulas, insert a clear placeholder (e.g., a comment indicating the original formula and its location, or a default value like `None` or `0`) in the generated Python script. Log a warning for each unsupported formula encountered.",
        "testStrategy": "Create test sheets containing known unsupported formulas. Verify that these formulas are identified during parsing and replaced with the defined placeholders in the generated script. Check that corresponding warnings are added to the log.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Develop Python Script Generation Core Logic",
        "description": "Develop the core logic for generating the runnable Python script from the extracted formulas and variable names.",
        "details": "Iterate through the extracted formulas and their corresponding variable names. Translate the formula logic (using the `formulas` library's internal representation or by mapping common functions) into Python code. Structure the script with input variable assignments (using names from Task 5), followed by derived variable calculations based on the formulas. Include necessary imports. Handle the order of calculations to respect dependencies between cells/formulas.",
        "testStrategy": "Generate scripts from test sheets with various formulas (arithmetic, logical, aggregations, lookups, cross-sheet). Manually review the generated Python code to ensure it accurately reflects the Excel logic, uses the correct variable names, and handles dependencies reasonably. Verify placeholders for unsupported formulas are present.",
        "priority": "high",
        "dependencies": [
          4,
          5,
          6,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement CLI Output Generation",
        "description": "Implement the output mechanism for the CLI, returning the generated Python script and the log file.",
        "details": "After script generation (Task 8) and logging (Task 13), write the generated Python code to standard output or a specified file path. Write the accumulated warnings and logs to `log.txt` in the current directory or a specified output directory. Handle error messages by writing to `stderr`.",
        "testStrategy": "Run the CLI with test files. Verify that the `.py` script is generated and output correctly (either to stdout or file). Check that the `log.txt` file is created and contains all expected warnings and messages. Test error output to `stderr`.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Cross-Sheet Reference Parsing",
            "description": "Modify the formula parsing component to detect and correctly parse cross-sheet references (e.g., SheetName!CellRef). This involves updating the lexer and parser to recognize sheet names and the '!' separator, and storing the sheet name and cell reference as distinct parts of the formula's Abstract Syntax Tree (AST) or intermediate representation. Ensure compatibility with various valid sheet naming conventions.",
            "dependencies": [],
            "details": "Update `formula_parser.py` and related components. Adhere to naming conventions and code organization standards. Implement comprehensive unit tests covering various valid and invalid cross-sheet reference formats (e.g., 'Sheet1!A1', 'Another Sheet'!B2, 'Sheet with Spaces'!C3, '!A1', 'Sheet1!'). Ensure error handling for malformed references is in place according to RULES.MD.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Build Sheet Dependency Graph",
            "description": "Based on the parsed cross-sheet references from Subtask 1, construct a directed graph representing dependencies between sheets. Each node in the graph should represent a sheet, and a directed edge from Sheet A to Sheet B indicates that Sheet A contains formulas referencing cells in Sheet B.",
            "dependencies": [
              1
            ],
            "details": "Develop a new module or integrate graph building logic into the processing pipeline. Use an appropriate graph data structure (e.g., adjacency list/matrix). Implement logic to iterate through all parsed formulas and identify cross-sheet dependencies. Adhere to architectural patterns. Implement unit tests for graph construction with different dependency scenarios (linear, branching, no dependencies).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Sheet Processing Order and Circular Dependency Detection",
            "description": "Implement a topological sort algorithm on the dependency graph built in Subtask 2 to determine the correct processing order for sheets. Integrate logic to detect circular dependencies during the topological sort process. If a circular dependency is found, handle it according to the specified error handling procedures (e.g., raise a specific exception, log the issue, potentially halt processing for that file).",
            "dependencies": [
              2
            ],
            "details": "Implement the topological sort algorithm (e.g., Kahn's algorithm or DFS-based). Ensure the algorithm correctly identifies cycles. Adhere to error handling and logging standards defined in RULES.MD. Implement unit tests specifically for topological sort and circular dependency detection using various graph structures, including test cases designed to trigger circular reference errors. Ensure this processing order is available for the code generation phase.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Generate Code with Cross-Sheet Reference Resolution",
            "description": "Modify the Python script generation component to correctly handle cross-sheet references based on the processing order determined in Subtask 3. This involves generating code that accesses data from sheets that have already been processed and made available (e.g., via generated variables or functions).",
            "dependencies": [
              3
            ],
            "details": "Update the code generation logic (`code_generator.py`). Define how data from processed sheets is stored and accessed by subsequent sheets in the generated script, ensuring security and performance thresholds are met. Implement test cases for code generation covering various cross-sheet reference scenarios, including references to different cell types and ranges. Integrate this into the end-to-end testing using the 5 benchmark Excel files, ensuring 90%+ unit test coverage for the new code. Verify the generated code correctly resolves references and produces expected outputs.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement REST API Output Generation",
        "description": "Implement the output mechanism for the REST API, returning the generated script, execution output (if run), and a log URL.",
        "details": "Define API response structure (JSON) including fields for the generated script content, execution results (if applicable, from Task 12), a list of warnings, and a URL/path to the log file. Serve the generated script and log file temporarily for download or include content directly in the JSON response where appropriate (e.g., warnings list).",
        "testStrategy": "Call the API endpoint with test files. Verify that the JSON response contains the generated script content, the warnings list, and a valid reference/URL for the log file. Test downloading the log file via the provided reference.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Centralized Logging Utility",
            "description": "Create a core Python logging utility class/module that supports different severity levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). Implement configuration options for output destinations (console, file) and log formatting, adhering to project naming conventions and code organization as per RULES.MD. Include initial unit tests for the utility's functionality, ensuring basic logging calls and output handling work correctly.",
            "dependencies": [],
            "details": "Based on FEATURES.MD (Logging requirements) and RULES.MD (Naming Conventions, Code Organization, Architectural Patterns, Testing Standards). The utility should be designed for easy integration and potential thread-safety. Focus on the core mechanism and configuration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate Logging Across All Components",
            "description": "Integrate the centralized logging utility (from Subtask 1) into all relevant components: File Ingestion, Formula Parsing, Python Script Generation, Secure Execution, and Interfaces. Ensure logging calls include sufficient contextual information (e.g., file name, cell reference, specific error/warning details) as required by FEATURES.MD. Implement robust error handling to log exceptions appropriately. Write integration tests to verify logging occurs correctly during component execution paths and that contextual information is captured.",
            "dependencies": [
              1
            ],
            "details": "Based on FEATURES.MD (all component details, Logging requirements) and RULES.MD (Error Handling, Performance Thresholds, Testing Standards). Pay close attention to performance impact during integration. Ensure logging covers critical steps and potential failure points in each component.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Log Analysis and Summary Generation & Comprehensive Testing",
            "description": "Develop functionality to read and analyze generated log files. Implement a mechanism to identify warnings and errors and generate a summary report (e.g., count by level, list of critical issues with context). Write unit tests for the analysis/summary logic. Conduct comprehensive testing, including using the 5 benchmark Excel files specified in RULES.MD, to verify the entire process, including log generation and summary accuracy. Ensure overall unit test coverage (including utility, integration, and analysis tests) reaches 90%+ as per RULES.MD.",
            "dependencies": [
              1,
              2
            ],
            "details": "Based on FEATURES.MD (Logging requirements, potentially reporting/interface needs) and RULES.MD (Testing Standards - 90%+ coverage, Benchmark Files, Error Handling). The summary should be easily consumable. Testing must cover various logging scenarios, verify log file contents, and validate the accuracy of summary reports against expected outcomes from benchmark files.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Centralized Logging Utility",
            "description": "Create a core Python logging utility with support for different severity levels (DEBUG, INFO, WARNING, ERROR). Configure handlers for console output and file output (`log.txt`). Define a standard log message format including timestamp, level, and message.",
            "dependencies": [],
            "details": "Define log levels and their usage guidelines. Set up the logging configuration using Python's `logging` module. Create a main logger instance accessible application-wide. Implement basic unit tests for the logger's configuration and output routing.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Logging Across Components with Context",
            "description": "Add logging calls to relevant parts of the application workflow (file ingestion, parsing, formula mapping, script generation, execution). Ensure log messages include contextual information such as sheet name, cell reference, formula string, or component name where applicable.",
            "dependencies": [
              4
            ],
            "details": "Identify key points for logging within each processing stage. Pass relevant contextual data (e.g., cell 'A1' on 'Sheet1') to log calls. Log events like file processing start/end, formula extraction, unsupported features (F5), naming decisions (F6, F10), script generation steps, and execution results/errors (F13). Verify logs are generated during component execution.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement Warning/Error Summary & Comprehensive Testing",
            "description": "Collect warnings and errors generated during a single conversion process run. Implement logic to format and output these warnings/errors according to requirements: to `log.txt`, `stderr` for CLI (F19), and as a `warnings[]` array in the API JSON response (F18). Define and implement comprehensive test cases to verify logging output for various scenarios and check the content and format of the generated log file.",
            "dependencies": [
              5
            ],
            "details": "Create a mechanism to aggregate warnings/errors per request. Implement output formatting logic for file, console, and JSON array. Develop test cases covering scenarios like file size limits (F3), unsupported formulas (F5), ambiguous headers (F10), cross-sheet reference issues (F7), and execution errors (F13). Write tests to assert the presence, content, and format of specific log messages in `log.txt`, `stderr`, and the API response `warnings[]` field.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 11,
        "title": "Setup Secure Script Execution Environment",
        "description": "Set up a secure environment for executing the generated Python scripts with resource limits and isolation.",
        "details": "Research and implement a method for sandboxing script execution. Options include Docker containers with restricted capabilities (AppArmor/SECCOMP) or subprocesses with OS-level resource limits (CPU time, memory, no network access). Configure the environment to enforce the specified limits (30s CPU, 128MB RAM).",
        "testStrategy": "Develop test scripts designed to exceed CPU time or memory limits, or attempt network access. Execute these scripts within the sandbox environment and verify that the resource limits are enforced and the script is terminated correctly without side effects or network access.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Naming Hierarchy and Fallback",
            "description": "Define and implement the hierarchy for generating variable names (e.g., header row, specific mapping row, default). Implement fallback strategies if the preferred source is unavailable or invalid.",
            "dependencies": [],
            "details": "Specify the order of preference for name sources. Define fallback rules (e.g., use column index if header is empty). Add test cases for missing headers, missing mapping entries, and combinations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Detect and Resolve Duplicate Headers",
            "description": "Implement logic to identify columns with identical header names within a single sheet and modify subsequent duplicates to ensure uniqueness.",
            "dependencies": [
              1
            ],
            "details": "Define the resolution strategy (e.g., appending a number or column index). Add test cases for multiple columns with the same header.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Handle Python Keywords and Reserved Names",
            "description": "Implement logic to check if a generated variable name is a Python keyword or reserved name and modify it to avoid conflicts.",
            "dependencies": [
              1,
              2
            ],
            "details": "Include a list of Python keywords and reserved names to check against. Define the modification strategy (e.g., appending an underscore). Add test cases for common keywords (e.g., 'class', 'def', 'if') after applying hierarchy and duplicate resolution.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Ensure Cross-Sheet Naming Consistency",
            "description": "Implement a mechanism to ensure that columns representing the same logical data across different sheets in the workbook are assigned consistent variable names.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Define how columns are matched across sheets (e.g., by header name, by a mapping). Implement a global registry or mapping to track names assigned to logical columns. Add test cases for matching columns across sheets with varying headers or positions, ensuring the final name respects keyword handling and duplicate resolution.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement Script Execution Logic",
        "description": "Implement the logic to execute the generated Python script within the secure sandbox environment and capture its output.",
        "details": "Take the generated script (from Task 8) and execute it using the sandboxing mechanism set up in Task 11. Capture standard output and standard error streams. Handle execution timeouts or resource limit violations gracefully. Format the captured output (e.g., print statements) into a structured format like JSON for the API or plain text for the CLI.",
        "testStrategy": "Generate scripts from test sheets that produce specific outputs via print statements. Execute these scripts in the sandbox and verify that the output is correctly captured and formatted. Test execution with scripts that should trigger resource limits or errors and ensure graceful handling.",
        "priority": "high",
        "dependencies": [
          8,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Test Excel Fixtures",
            "description": "Design and create a set of Excel files (.xlsx, .csv, .tsv) containing various formula types, cross-sheet references, named ranges, headers, and edge cases with documented expected outputs.",
            "dependencies": [],
            "details": "Create test files covering arithmetic (+, -, *, /), logical (IF, AND, OR), aggregation (SUM, AVERAGE, COUNT), and lookup (VLOOKUP, HLOOKUP) formulas. Include specific sheets/cases for cross-sheet references (F7), named ranges (F6), ambiguous headers (F10), and edge cases (e.g., errors, empty cells, unsupported formulas F5). Document the expected Python code output and/or execution results for each test case. Aim for at least 5 benchmark files as specified in RULES.md.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Tests for Basic Formula Types",
            "description": "Write tests to verify the correct conversion of basic arithmetic, logical, aggregation, and lookup formulas from Excel to Python using the created test fixtures.",
            "dependencies": [
              1
            ],
            "details": "Use the fixtures from Subtask 1. Implement tests that parse the Excel files (F1, F2, F4) and assert that the generated Python code (F8) correctly represents the original formula logic. Verification methods include comparing generated Python code snippets against expected patterns and potentially executing the generated code in a sandboxed environment (F12, F13) and comparing results against expected values documented in the fixtures. Ensure coverage for F5 (unsupported placeholders) and F6 (variable naming from headers/named ranges).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Tests for Cross-Sheet References and Edge Cases",
            "description": "Write tests specifically targeting cross-sheet references, named ranges, ambiguous headers, and other edge cases identified in the fixtures.",
            "dependencies": [
              1,
              2
            ],
            "details": "Use the fixtures from Subtask 1 and the testing framework from Subtask 2. Implement tests to verify correct handling of cross-sheet references (F7), variable naming from named ranges/headers (F6, F10), and graceful handling/logging of edge cases like unsupported formulas (F5) or errors (F17, F18, F19). Verification methods include checking generated Python code for correct references/variable names and verifying log/warning outputs against expected messages.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement CLI Interface Tests",
            "description": "Write tests to verify the functionality of the command-line interface, including file ingestion, processing, and output generation.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Use the test fixtures from Subtask 1 and the core conversion logic tested in Subtasks 2 and 3. Implement tests that invoke the CLI (F14) with different file types (.xlsx, .csv, .tsv - F1) and options (e.g., `--run` F12). Verification methods include checking the CLI output (stdout, stderr - F19), verifying that the generated Python file (F16) is created correctly, and potentially comparing execution results if the `--run` option is used (F12, F13). Test file size limits (F3).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement API Interface Tests",
            "description": "Write tests to verify the functionality of the REST API interface, including file upload, processing, and JSON response generation.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Use the test fixtures from Subtask 1 and the core conversion logic tested in Subtasks 2 and 3. Implement tests that send file POST requests to the API endpoint (F15). Verification methods include checking the HTTP response status code (e.g., 200 OK), verifying the structure and content of the JSON response (F15), including the warnings array (F18) and the generated Python code/download link (F16). Test file size limits (F3) and concurrency handling (F21).",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 13,
        "title": "Develop Logging and Warning System",
        "description": "Develop a comprehensive logging and warning system to record parsing issues, unsupported formulas, naming fallbacks, and execution details.",
        "details": "Use Python's built-in `logging` module. Define different logging levels (INFO, WARNING, ERROR). Implement logging points throughout the parsing, naming, generation, and execution phases to capture relevant events, especially warnings about skipped elements, naming ambiguities, and unsupported features as specified in the PRD. Ensure logs can be written to a file (`log.txt`).",
        "testStrategy": "Process test files designed to trigger various warnings (e.g., empty headers, unsupported formulas). Verify that the logging system captures these events correctly with appropriate messages and levels. Check that the log file (`log.txt`) is generated and contains the expected entries.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Unit Tests for Core Components",
        "description": "Write unit tests for key components: formula extraction, variable naming logic, and Python code generation.",
        "details": "Create isolated test cases for the functions responsible for parsing formulas using `formulas`, applying the variable naming rules, and generating Python code snippets. Use mock data representing parsed sheet structures and extracted formulas. Ensure tests cover edge cases and different scenarios outlined in the PRD.",
        "testStrategy": "Run the unit test suite. Ensure all tests pass and achieve high code coverage for the core parsing, naming, and generation modules. Verify tests cover successful cases, edge cases, and expected error/warning conditions.",
        "priority": "high",
        "dependencies": [
          4,
          5,
          8,
          13
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Develop End-to-End Test Suite with Benchmarks",
        "description": "Develop an end-to-end test suite using benchmark Excel test cases with known expected outputs.",
        "details": "Select or create 5 benchmark Excel files representing real-world complexity (finance models, analytics sheets). For each benchmark, determine the expected output values after formula calculation. Implement tests that upload/process these files, generate and execute the script (Task 12), and compare the actual output against the known expected output (using checksum/hash validation of output JSONs or direct value comparison).",
        "testStrategy": "Run the end-to-end test suite with the 5 benchmark files. Verify that the generated scripts execute successfully within the sandbox and produce outputs that match the pre-calculated expected results for each benchmark. Check that logs are generated correctly.",
        "priority": "high",
        "dependencies": [
          8,
          12,
          13
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Request Queue and Worker Pool",
            "description": "Set up an asynchronous request queue and a pool of workers (threads or processes) to handle incoming file conversion requests.",
            "dependencies": [],
            "details": "Use `asyncio` queues or similar mechanisms provided by FastAPI/Python. Define worker logic to pick up tasks from the queue. Consider using `concurrent.futures` or `asyncio` workers.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate Semaphore for Concurrency Control",
            "description": "Implement a semaphore or similar mechanism (e.g., `asyncio.Semaphore`) to limit the maximum number of concurrently executing conversion tasks to prevent resource exhaustion, aligning with NFR F21.",
            "dependencies": [
              1
            ],
            "details": "Configure the semaphore limit based on expected load and available resources. Ensure workers acquire the semaphore before starting a task and release it upon completion.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Adapt Conversion Logic for Background Processing",
            "description": "Modify the existing file ingestion, parsing, and script generation logic to run as non-blocking background tasks submitted to the worker pool. Include test cases for concurrent uploads and high-load scenarios.",
            "dependencies": [
              1,
              2
            ],
            "details": "Refactor the core conversion pipeline into a function suitable for execution by a worker. Add unit and integration tests specifically designed to simulate concurrent requests and high throughput.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Conversion Status Tracking",
            "description": "Develop a system to track the status (e.g., queued, processing, completed, failed) of each submitted conversion request and potentially provide a mechanism for reporting this status.",
            "dependencies": [
              3
            ],
            "details": "Use an in-memory dictionary, a simple database, or a dedicated task queue system (if needed) to store the state of each request identified by a unique ID. Define states and transitions.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 16,
        "title": "Add Concurrency Handling for API",
        "description": "Implement concurrency handling for the REST API to support multiple simultaneous file uploads and processing requests.",
        "details": "Leverage FastAPI's asynchronous capabilities. Ensure that file processing, script generation, and execution logic are non-blocking or handled in background tasks/worker processes to allow the API to handle 10 concurrent requests/minute. Consider using a task queue if processing is lengthy.",
        "testStrategy": "Use a load testing tool (e.g., `locust`, `ab`) to simulate 10+ concurrent API requests with file uploads. Monitor the API's response time and error rate. Verify that the system remains responsive and correctly processes concurrent requests without data corruption or significant performance degradation.",
        "priority": "medium",
        "dependencies": [
          2,
          10,
          12
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze Blocking Operations",
            "description": "Identify specific blocking I/O or CPU-bound operations within the existing file upload, parsing, script generation, and execution flow that prevent concurrency.",
            "dependencies": [],
            "details": "Review existing code for synchronous file reads/writes, blocking calls to external processes (script execution), and CPU-intensive parsing/generation logic. Document identified bottlenecks and potential areas for asynchronous implementation or offloading.",
            "status": "pending",
            "testStrategy": "N/A (Analysis task)"
          },
          {
            "id": 2,
            "title": "Refactor FastAPI Endpoint for Async Upload",
            "description": "Modify the `/upload` endpoint to use `async def` and handle file streaming asynchronously, ensuring non-blocking file reception.",
            "dependencies": [],
            "details": "Update the FastAPI endpoint definition to be asynchronous. Implement asynchronous file handling using `asyncio` compatible libraries or FastAPI's built-in async file handling capabilities. Ensure proper error handling for upload issues.",
            "status": "pending",
            "testStrategy": "Unit tests for async file handling logic. Integration tests for the `/upload` endpoint to verify non-blocking behavior during file reception."
          },
          {
            "id": 3,
            "title": "Implement Asynchronous File Ingestion/Parsing",
            "description": "Adapt the logic for reading and parsing uploaded files (e.g., Excel) to use asynchronous libraries or patterns, avoiding blocking calls within the API process.",
            "dependencies": [
              1,
              2
            ],
            "details": "Refactor file reading and initial parsing logic (e.g., using `aiofiles` for file I/O, or adapting parsing libraries if async versions exist, or offloading parsing). Ensure this step is non-blocking within the main API event loop.",
            "status": "pending",
            "testStrategy": "Unit tests for asynchronous file reading and parsing functions (90%+ coverage). Integration tests verifying async parsing doesn't block the API."
          },
          {
            "id": 4,
            "title": "Select and Integrate Task Queue",
            "description": "Choose a suitable task queue (e.g., Celery with Redis/RabbitMQ) based on project needs and integrate it into the project structure.",
            "dependencies": [],
            "details": "Evaluate task queue options considering reliability, scalability, ease of integration with FastAPI, and monitoring capabilities. Set up the chosen task queue broker and backend. Integrate the task queue client library into the project structure, adhering to code organization standards.",
            "status": "pending",
            "testStrategy": "Unit tests for task queue configuration and initialization. Basic integration test verifying connection to the broker/backend."
          },
          {
            "id": 5,
            "title": "Create Task Definitions for Processing/Execution",
            "description": "Define specific tasks within the task queue system for formula parsing, Python script generation, and secure script execution.",
            "dependencies": [
              3,
              4
            ],
            "details": "Wrap the existing (or refactored) logic for formula parsing, script generation, and secure execution into functions that can be registered as tasks with the chosen task queue. Define task parameters and return types.",
            "status": "pending",
            "testStrategy": "Unit tests for each task function to ensure core logic works correctly in isolation. Integration tests verifying tasks can be registered and called via the task queue client."
          },
          {
            "id": 6,
            "title": "Develop Task Queue Worker Processes",
            "description": "Implement worker processes that connect to the task queue, retrieve tasks, and execute the defined processing and execution logic securely and reliably.",
            "dependencies": [
              4,
              5
            ],
            "details": "Set up worker processes that listen to the task queue. Configure workers to handle task execution, including error handling, logging, and resource management. Ensure secure execution environment is maintained within workers.",
            "status": "pending",
            "testStrategy": "Integration tests verifying workers can consume tasks and execute them successfully. Test error handling within workers for task failures."
          },
          {
            "id": 7,
            "title": "Modify API to Enqueue Tasks and Return Task ID",
            "description": "Update the `/upload` or processing endpoint to enqueue the necessary tasks to the queue and immediately return a unique task ID to the client.",
            "dependencies": [
              3,
              6
            ],
            "details": "After initial async file ingestion/parsing (if done in API), call the task queue client to enqueue the subsequent processing/execution tasks. Capture the returned task ID and include it in the API response to the client.",
            "status": "pending",
            "testStrategy": "Integration tests for the API endpoint verifying tasks are correctly enqueued and a valid task ID is returned in the response."
          },
          {
            "id": 8,
            "title": "Implement Task Status Monitoring and Retrieval",
            "description": "Develop logic to store and retrieve the status (e.g., pending, processing, completed, failed) and results/errors of tasks using the task queue's backend or a separate store.",
            "dependencies": [
              4,
              6
            ],
            "details": "Utilize the task queue's built-in result backend or implement a separate mechanism (e.g., database table) to store task status, results, and error information associated with the task ID. Implement functions to query this information.",
            "status": "pending",
            "testStrategy": "Unit tests for status storage and retrieval logic. Integration tests verifying task status is correctly updated and retrievable after task completion or failure."
          },
          {
            "id": 9,
            "title": "Create API Endpoint for Status Check",
            "description": "Add a new API endpoint (e.g., `/status/{task_id}`) that allows clients to query the status and results of their submitted tasks using the returned task ID.",
            "dependencies": [
              7,
              8
            ],
            "details": "Implement a new FastAPI endpoint that accepts a task ID as a path parameter. Use the task status monitoring logic (Subtask 8) to retrieve the current status and results/errors for the given ID and return it in a structured API response.",
            "status": "pending",
            "testStrategy": "Integration tests for the `/status/{task_id}` endpoint, verifying it returns correct status for pending, processing, completed, and failed tasks using benchmark files."
          },
          {
            "id": 10,
            "title": "Implement Concurrent Load Testing and Optimization",
            "description": "Design and execute load tests simulating 10 concurrent requests/minute using the 5 benchmark Excel files, analyze performance metrics, identify bottlenecks in the concurrent flow, and optimize the implementation.",
            "dependencies": [
              7,
              9
            ],
            "details": "Use a load testing tool (e.g., Locust, JMeter) to simulate the required load. Test the full flow from upload/enqueue to status check. Monitor API response times, task queue performance, worker resource usage, and error rates. Optimize code or infrastructure based on results to meet performance thresholds.",
            "status": "pending",
            "testStrategy": "Performance tests using load testing tools with 5 benchmark files, targeting 10 concurrent requests/minute. Analyze results against performance requirements."
          }
        ]
      },
      {
        "id": 17,
        "title": "Implement Robust Malformed File Handling",
        "description": "Implement robust handling for malformed, corrupted, or password-protected input files.",
        "details": "Add checks during the file upload (Task 2) and parsing (Task 3) stages to detect invalid file formats, corruption, or password protection. Gracefully reject such files, return appropriate error messages via CLI/API, and log the event. Use libraries like `openpyxl` or `pandas` error handling for this.",
        "testStrategy": "Prepare test files that are corrupted, password-protected, or have incorrect extensions/formats. Attempt to upload and process these files via CLI and API. Verify that the system rejects them gracefully, provides clear error messages to the user, and logs the rejection.",
        "priority": "medium",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Malformed File Error Types",
            "description": "Define specific error codes or types within the application's error handling framework to represent different kinds of malformed files (e.g., invalid format, corrupted content, password protected).",
            "dependencies": [],
            "details": "Create distinct error classes or enums for 'InvalidFileFormatError', 'FileCorruptionError', and 'PasswordProtectedError'. Ensure these integrate with the existing error handling architecture as per RULES.MD.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement File Format Validation",
            "description": "Implement logic to validate the file format during the initial upload stage (related to Task 2). This should check file extensions and potentially basic file headers/magic bytes to ensure it's a likely Excel file (.xlsx).",
            "dependencies": [
              1
            ],
            "details": "Add checks in the file upload handler to verify the uploaded file's extension and potentially perform a basic check on the file's content signature if feasible and performant. If validation fails, raise the appropriate error type defined in Subtask 1.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement File Corruption Detection",
            "description": "Implement logic to detect file corruption during the initial read or loading process using library features (e.g., `openpyxl`'s error handling during load).",
            "dependencies": [
              1
            ],
            "details": "Wrap the file loading operation (e.g., `openpyxl.load_workbook`) in a try-except block to catch exceptions indicative of file corruption. If a corruption error is detected, raise the appropriate error type defined in Subtask 1.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Password Protection Detection",
            "description": "Implement logic to detect if an uploaded file is password protected during the initial read or loading process using library features.",
            "dependencies": [
              1
            ],
            "details": "Use library features (e.g., checking specific exceptions raised by `openpyxl` or `pandas` when trying to open a password-protected file) to identify password protection. If detected, raise the appropriate error type defined in Subtask 1.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Checks into File Upload Handler",
            "description": "Integrate the format validation (Subtask 2), corruption detection (Subtask 3), and password protection detection (Subtask 4) into the main file upload processing flow (Task 2).",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Modify the file upload handler to sequentially perform the checks implemented in Subtasks 2, 3, and 4. If any check fails, the process should stop, and the detected error should be propagated.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Refine Parsing Stage Error Handling",
            "description": "Enhance the error handling within the file parsing logic (Task 3) to gracefully handle errors that might occur during detailed parsing, even if initial checks passed (e.g., invalid cell data, structural issues within valid format).",
            "dependencies": [
              1,
              5
            ],
            "details": "Review and update the parsing code (Task 3) to include robust try-except blocks around operations that might fail due to unexpected data or structure. Map these parsing-specific errors to appropriate internal error types, potentially extending the types defined in Subtask 1 if needed.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement CLI Error Reporting for Malformed Files",
            "description": "Design and implement the mechanism for reporting malformed file errors to the user via the Command Line Interface (CLI).",
            "dependencies": [
              5,
              6
            ],
            "details": "Modify the CLI interface to catch the specific malformed file errors propagated from the upload (Subtask 5) and parsing (Subtask 6) stages. Display clear, user-friendly error messages indicating the type of issue (invalid format, corrupted, password protected) and that the file was rejected.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement API Error Reporting for Malformed Files",
            "description": "Design and implement the mechanism for reporting malformed file errors via the API interface.",
            "dependencies": [
              5,
              6
            ],
            "details": "Modify the API endpoints handling file upload/processing to catch the specific malformed file errors. Return appropriate HTTP status codes (e.g., 400 Bad Request) and a structured JSON response body detailing the error type and message, adhering to API error standards.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement Malformed File Event Logging",
            "description": "Implement detailed logging for all instances where a malformed, corrupted, or password-protected file is detected and rejected.",
            "dependencies": [
              5,
              6
            ],
            "details": "Add logging statements at the points where malformed file errors are caught and handled (Subtasks 5, 6). Log the event timestamp, the type of error (format, corruption, password), the file name (if available and safe to log), and potentially a request ID or user ID for traceability, following the application's logging standards.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Develop Comprehensive Tests for Malformed File Handling",
            "description": "Develop unit and integration tests to cover all aspects of malformed file handling, including format validation, corruption detection, password protection detection, error propagation, CLI/API reporting, and logging.",
            "dependencies": [
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Create test cases using intentionally malformed, corrupted, and password-protected files. Include tests verifying that the 5 benchmark Excel files are correctly processed without triggering these errors. Ensure tests cover error handling paths in upload and parsing, verify correct CLI/API output/responses, and check for appropriate log entries. Aim for 90%+ unit test coverage for the implemented logic.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-07-07T16:48:07.153Z",
      "updated": "2025-07-07T18:48:44.474Z",
      "description": "Tasks for master context"
    }
  }
}