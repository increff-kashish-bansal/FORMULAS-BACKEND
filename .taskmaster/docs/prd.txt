# Product Requirements Document (PRD): Excel-to-Python Formula Converter

## Overview

This project aims to help data analysts, finance professionals, and client teams transition away from Excel by providing a tool that converts uploaded Excel files into clean, runnable Python scripts. The system leverages [`xlcalculator`](https://github.com/bradbase/xlcalculator) to parse and evaluate Excel formulas internally, and uses that evaluation to generate Python code where possible. For formulas that cannot be statically translated, `xlcalculator` will be used as a runtime evaluator.

## Goals and Objectives

* **Enable seamless migration from Excel to Python** by extracting and evaluating formula logic.
* **Reduce dependency on manual Excel workflows**, enhancing reproducibility and transparency.
* **Deliver a CLI/API-based MVP** for internal or early client usage.
* **Leverage `xlcalculator` to reduce implementation effort**, while still outputting human-readable Python code where feasible.

## Scope

### In Scope (MVP)

* Upload support for `.xlsx`, `.csv`, and `.tsv` files
* Parse and evaluate **formulas only** from all sheets in the workbook
* Use `xlcalculator` to:

  * Evaluate formulas internally
  * Extract dependency relationships for generating readable output code
* Auto-generate a **runnable Python script**, optionally using `xlcalculator` API calls for dynamic formula resolution
* Provide CLI and REST API interfaces for file upload, code generation, and evaluation
* Allow generated code to be **executed within the application** securely
* Graceful handling of unsupported or complex formulas via placeholder comments or direct `xlcalculator` bindings

### Out of Scope (MVP)

* UI/frontend for uploads or code display
* Handling of formatting, charts, VBA/macros
* Partial sheet or range-specific conversion
* Security or encryption features

## User Personas / Target Audience

1. **Data Analysts**: Users working in Excel who need repeatable, auditable Python code
2. **Finance Professionals**: Spreadsheet-heavy users seeking automation for models
3. **Client Teams**: Looking to integrate spreadsheet logic into their backend systems

## Functional Requirements (MoSCoW Prioritization)

### Must Have

* Accept `.xlsx`, `.csv`, `.tsv` files via CLI or REST endpoint
* Use `xlcalculator` to parse formulas and evaluate values
* Generate `.py` file output with:

  * Placeholder substitution for unsupported logic
  * Variable names inferred from headers or labels

### Should Have

* Cross-sheet reference support
* Logging and warnings for skipped elements or evaluated fallbacks
* Consistent naming scheme for variables (aligned with Excel naming)

### Could Have

* Option to keep formulas live via `xlcalculator` rather than translating to static Python expressions
* Execution engine that runs the generated script and returns evaluated outputs

### Won’t Have (Now)

* UI/Frontend
* Full fidelity support for Excel formatting, charts, macros, or VBA
* Partial sheet/range selection or visual formula editing

## Non-Functional Requirements

* **Performance**: Should process workbooks <10MB within 10 seconds
* **Scalability**: Handle 10+ concurrent uploads/minute via async FastAPI
* **Maintainability**: Modular, testable Python architecture with logs and structured error handling
* **Deployment**: Azure-hosted VM/App Service
* **Security**: Sandbox execution

  * Max 30s CPU, 128MB RAM, no internet access
  * Docker with AppArmor/SECCOMP or subprocess with `resource` limits

## Technical Architecture

### CLI/REST Flow

* User uploads a file (CLI/REST)
* Server stores and validates file
* Backend uses `xlcalculator` to evaluate the workbook

### Processing Pipeline

1. Validate file type and size
2. Load file and extract workbook structure
3. Use `xlcalculator` to evaluate and resolve formulas
4. Generate Python code by combining static translation and embedded runtime calls (if needed)

### Execution Flow

* Evaluate via `exec()` in sandboxed subprocess or container
* Return stdout/stderr (CLI) or structured JSON (API)
* Include downloadable `log.txt` with warnings, skipped cells, or runtime fallbacks

## Variable Naming Logic

* Hierarchy: Named Range > Header > Cell Reference (e.g., `cell_A1`)
* Fallbacks used for missing/merged/malformed headers
* Logs include clarification when variables are inferred from cell addresses

## Supported Formula Types (MVP)

* All formulas supported by `xlcalculator`, including:

  * Math: `SUM`, `PRODUCT`, `ROUND`, etc.
  * Logic: `IF`, `AND`, `OR`, `NOT`
  * Lookup: `VLOOKUP`, `INDEX`, `MATCH`
  * String/date/time functions (partial)

## Known Limitations

* Generated Python code may fall back to runtime `xlcalculator` API for complex expressions
* No AST-based translation; semantic gaps may remain between Excel logic and output Python
* Macros/VBA, charts, and visual formatting are not interpreted
* External links, shared workbooks, and password protection are unsupported

## Output Format & Example

* Outputs `.py` script per uploaded file
* Static Python where possible, embedded runtime evaluation where needed:

```python
from xlcalculator import ModelCompiler, Evaluator

# Load model from .xlsx
model = ModelCompiler().read_and_parse_archive("my_sheet.xlsx")
evaluator = Evaluator(model)

# Example static inputs
revenue = 10000
cost = 7000

# Runtime formula (if too complex for direct translation)
profit = evaluator.evaluate("Sheet1!C2")  # Assuming C2 contains =A2-B2
print("Profit:", profit)
```

## Warning/Error Output

* CLI: stderr + `log.txt`
* API: JSON with `warnings[]`, `log_url`, and execution output summary

## User Journey

1. User uploads a file via CLI/API
2. Backend processes and evaluates it using `xlcalculator`
3. System generates `.py` script — static and/or runtime-evaluated
4. Output is returned and optionally executed
5. Logs and evaluation results are returned/downloadable

## Success Metrics

* 90%+ formula coverage across real-world test files
* <10s processing time for files <10MB
* 95%+ API reliability (HTTP 200 within 3s)
* 0 sandbox escapes or security incidents in staging
* Matched output vs Excel reference for 5 benchmark sheets

## Testing and QA Plan

* Unit tests for file handling, evaluation, and naming logic
* Integration tests for `.xlsx`, `.csv`, `.tsv` input files
* End-to-end: load file → evaluate → generate → run → compare
* Benchmark: hashed/expected output comparison for 5 client-style sheets
* Error path tests: malformed, passworded, empty headers

## Timeline

| Milestone                        | Target Date |
| -------------------------------- | ----------- |
| MVP CLI/API core logic           | July 7      |
| Azure deployment + execution API | July 7      |
| Client testing & feedback loop   | July 8-10   |
| Enhancements post feedback       | July 11+    |

## Open Questions / Assumptions

* Output code will mix static translation and runtime evaluation when needed
* All sheets will be parsed — selective conversion is deferred
* No frontend/UI planned for MVP

---

**Prepared by:** ChatGPT (Product PM)
**Date:** July 7, 2025
